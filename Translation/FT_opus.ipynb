{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FT_opus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tune the opus-mt model"
      ],
      "metadata": {
        "id": "it78SLwdXFkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI9LvMU6XCHW"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from datasets import Dataset\n",
        "from datasets import DatasetDict\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pickle\n",
        "import re"
      ],
      "metadata": {
        "id": "IUB8TGjBXK-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/export/home/cse200093/wmt_biomed/2016/pubmed_en_fr.txt' # txt file containing corresponding en and fr texts\n",
        "path2 = '/export/home/cse200093/wmt_biomed/2019/train/fr-en/' # folder"
      ],
      "metadata": {
        "id": "llU4juEBXRMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load metric offline\n",
        "# bleu.py requires bleu1.py to run\n",
        "metric = load_metric(\"/export/home/cse200093/Expe_Translation/bleu.py\")\n",
        "metric"
      ],
      "metadata": {
        "id": "vPM9D_NyXV79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load wmt_biomed/2016\n",
        "en = []\n",
        "fr = []\n",
        "ids = []\n",
        "file1 = open(path1, 'r')\n",
        "lines = file1.readlines()\n",
        "for line in tqdm(lines):\n",
        "    sent_id = line.split('|')[0]\n",
        "    sent_en = line.split('|')[1]\n",
        "    sent_fr = line.split('|')[2]\n",
        "    if sent_en != '[Not Available].':\n",
        "        en.append(sent_en.replace('[','').replace(']',''))\n",
        "        fr.append(sent_fr.rstrip('\\n'))\n",
        "        ids.append(sent_id)"
      ],
      "metadata": {
        "id": "8SPJZlzGXXpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.DataFrame({'id':ids, 'English':en, 'French':fr})\n",
        "df1"
      ],
      "metadata": {
        "id": "fmrf3bNWXdds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load wmt_biomed/2019\n",
        "# select all ann files containing annotations\n",
        "os.chdir(r'/export/home/cse200093/wmt_biomed/2019/train/fr-en') # eng\n",
        "eng_files = glob.glob('*_en.txt')\n",
        "\n",
        "os.chdir(r'/export/home/cse200093/wmt_biomed/2019/train/fr-en') # fre\n",
        "fre_files = glob.glob('*_fr.txt')"
      ],
      "metadata": {
        "id": "gtUa5nUCXeoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_files.sort()\n",
        "fre_files.sort()"
      ],
      "metadata": {
        "id": "P8MGuuszXivL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en1 = []\n",
        "fr1 = []\n",
        "ids1 = []\n",
        "#pattern= r'\\b[A-ZÀ-Ÿ]+(?:\\s+[A-ZÀ-Ÿ]+)*\\b'\n",
        "for file in tqdm(eng_files):\n",
        "    f = open(path2+file, 'r')\n",
        "    #print([x for x in re.findall(pattern, f.read()) if len(x)>6])\n",
        "    #print(f.read().split(\"\\n\",2)[2])\n",
        "    en1.append(f.read().split(\"\\n\",2)[2])\n",
        "#     lines = f.readlines()\n",
        "#     ids1.append(lines[0].rstrip('\\n').split(' ')[-1])\n",
        "#     # lines[1] are authors \n",
        "#     for line in lines[2:]:\n",
        "#         line = line.rstrip('\\n')\n",
        "#         en1.append(line)\n",
        "    f.close()\n",
        "    \n",
        "for file in tqdm(fre_files):\n",
        "    f = open(path2+file, 'r')\n",
        "    #print([x for x in re.findall(pattern, f.read()) if len(x)>5])\n",
        "    #print(f.read().split(\"\\n\",2)[2])\n",
        "    fr1.append(f.read().split(\"\\n\",2)[2])\n",
        "#     lines = f.readlines()\n",
        "#     # lines[1] are authors \n",
        "#     for line in lines[2:]:\n",
        "#         line = line.rstrip('\\n')\n",
        "#         fr1.append(line)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "GwgH2l9HXkST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {\"en\": en+en1, \"fr\": fr+fr1}\n",
        "\n",
        "dataset = Dataset.from_dict(my_dict)\n",
        "dataset"
      ],
      "metadata": {
        "id": "524VAAqXXlvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test validation split\n",
        "train_dataset, test_dataset= dataset.train_test_split(test_size=0.1).values()\n",
        "train_dataset, validation_dataset= train_dataset.train_test_split(test_size=0.1).values()\n",
        "raw_datasets = DatasetDict({\"train\":train_dataset,\"validation\":validation_dataset,\"test\":test_dataset})\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "QITvmRZLXoAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# save raw datasets\n",
        "open_file = open('/export/home/cse200093/Expe_Translation/raw_datasets_wmt_biomed_2016_2019.pkl', \"wb\")\n",
        "pickle.dump(raw_datasets, open_file)\n",
        "open_file.close()"
      ],
      "metadata": {
        "id": "WM85bK4_X2a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load raw_datasets\n",
        "open_file = open('raw_datasets_wmt_biomed_2016_2019.pkl', \"rb\")\n",
        "raw_datasets = pickle.load(open_file)\n",
        "open_file.close()\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "SXPmI5jPX3Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "# The model we want to fine-tune\n",
        "model_checkpoint = '/export/home/cse200093/opus-mt-fr-en'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "55FnI9lZX5uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_source_length = 128\n",
        "max_target_length = 128\n",
        "source_lang = \"fr\"\n",
        "target_lang = \"en\"\n",
        "\n",
        "def batch_tokenize_fn(examples):\n",
        "    \"\"\"\n",
        "    Generate the input_ids and labels field for huggingface dataset/dataset dict.\n",
        "    \n",
        "    Truncation is enabled, so we cap the sentence to the max length, padding will be done later\n",
        "    in a data collator, so pad examples to the longest length in the batch and not the whole dataset.\n",
        "    \"\"\"\n",
        "    sources = examples[source_lang]\n",
        "    targets = examples[target_lang]\n",
        "    model_inputs = tokenizer(sources, max_length=max_source_length, truncation=True)\n",
        "\n",
        "    # setup the tokenizer for targets,\n",
        "    # huggingface expects the target tokenized ids to be stored in the labels field\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "T97E9CsOX7Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(batch_tokenize_fn, batched=True)"
      ],
      "metadata": {
        "id": "uPFHdQ3NX9CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# solve device problem\n",
        "class torch_global:\n",
        "    device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    class set_device(object):\n",
        "        def __init__(self, device, error='ignore'):\n",
        "            try:\n",
        "                count = torch.cuda.device_count()\n",
        "                print(f'Available CUDA devices: {count}')\n",
        "            except:\n",
        "                print('No available CUDA devices')\n",
        "                #logger.error('No available CUDA devices')\n",
        "            self.previous = torch_global.device\n",
        "            try:\n",
        "                new_device = torch.device(device) if isinstance(device, str) else device\n",
        "                torch.as_tensor([0]).to(new_device)\n",
        "            except:\n",
        "                msg = f\"Device {device} is not available\"\n",
        "                if error == \"ignore\":\n",
        "                    print(msg)\n",
        "                else:\n",
        "                    raise\n",
        "            else:\n",
        "                torch_global.device = new_device\n",
        "            print(f'Current device: {torch_global.device}')\n",
        "\n",
        "        def __enter__(self):\n",
        "            pass\n",
        "\n",
        "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "            torch_global.device = self.previous\n"
      ],
      "metadata": {
        "id": "axUf3HVEX-li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_global.set_device('cuda:6')\n",
        "device = torch_global.device"
      ],
      "metadata": {
        "id": "KOi2hmw8YAmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model we want to fine-tune\n",
        "model_checkpoint = '/export/home/cse200093/opus-mt-fr-en'\n",
        "# model_checkpoint = \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
        "# model.to(device)\n",
        "print('# of parameters: ', model.num_parameters())"
      ],
      "metadata": {
        "id": "Hhw4rnC0YB9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to generate translation for a model\n",
        "def generate_translation(model, tokenizer, example):\n",
        "    \"\"\"print out the source, target and predicted raw text.\"\"\"\n",
        "    source = example[source_lang]\n",
        "    target = example[target_lang]\n",
        "    input_ids = example['input_ids']\n",
        "    input_ids = torch.LongTensor(input_ids).view(1, -1).to(model.device)\n",
        "    generated_ids = model.generate(input_ids)\n",
        "    prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    print('source: ', source)\n",
        "    print('target: ', target)\n",
        "    print('prediction: ', prediction)"
      ],
      "metadata": {
        "id": "avvCCfR1YEND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = tokenized_datasets['train'][1]\n",
        "generate_translation(model, tokenizer, example)"
      ],
      "metadata": {
        "id": "BMuvz-pDYKK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"/export/home/cse200093/Expe_Translation/{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=15, # to change\n",
        "    predict_with_generate=True,\n",
        "    remove_unused_columns=True,\n",
        "    fp16=True,\n",
        "    #push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ERfvG8rNYL9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ],
      "metadata": {
        "id": "a7mGZzgzYNT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "rrGqJdrPYOuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    #compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "2dTWDLAlYQDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start fine tuning\n",
        "trainer_output = trainer.train(\"opus-mt-fr-en-finetuned-fr-to-en/checkpoint-28000\")\n",
        "trainer_output"
      ],
      "metadata": {
        "id": "WDh_IrF5YRVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save fine tuned model\n",
        "model.save_pretrained(\"/export/home/cse200093/Expe_Translation/opus-mt-fr-en-finetuned-fr-to-en/FT_opus_model\")"
      ],
      "metadata": {
        "id": "_AxHrDpcYSzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easynmt import EasyNMT, models\n",
        "# load the model after FT :\n",
        "model_fr_en = EasyNMT(translator = models.AutoModel('/export/home/cse200093/Expe_Translation/opus-mt-fr-en-finetuned-fr-to-en/FT_opus_model'))"
      ],
      "metadata": {
        "id": "OaJ6pqWiYamC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the model after FT to translate\n",
        "model_fr_en.translate('''lupus diagnostiqué à l’âge de 13 ans avec atteinte pleuro-péricardique et du système nerveux central pris\n",
        "en charge à Bordeaux : 6 bolus d’ENDOXAN relayés par corticoïdes et PLAQUENIL pendant environ 2 ans.\n",
        "Pas d’atteinte rénale. Plus de traitement depuis l’âge de 16 ans. Patiente revue en consultation à Paris à\n",
        "Cochin par le Docteur BOINI en 2011 : pas de signe clinique d’activité lupique.\n",
        "Sur le plan immunologique : FAN 1/180 sans spécificité, anti-DNA négatif, anticorps anti-ECT négatif,\n",
        "anticorps anti-cardiolipine positifs à 29 unités mais pas d’anti-bêta 2 GPI et pas d’anti-coagulant circulant.\n",
        "Sédiment urinaire calme. Pas de traitement spécifique mis en place.''',source_lang = 'fr',target_lang='en')"
      ],
      "metadata": {
        "id": "XvNKz4oyYdjs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}